{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c1d0b4-58b9-4a3b-abd5-9ae7adf90daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mmcv\n",
    "from numpy.linalg import norm, pinv\n",
    "from scipy.special import softmax\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from os.path import basename, splitext\n",
    "from scipy.special import logsumexp\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb665ad-e0f5-4d64-8f9c-add27b480d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Say hello')\n",
    "    parser.add_argument('fc', help='Path to config')\n",
    "    parser.add_argument('train_feature', help='Path to data used to train')\n",
    "    parser.add_argument('feature_to_detect_data', nargs=\"+\", help='Path to data going to be detected')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def kl(p, q):\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0f8b4b-148e-40a9-8833-94cd4254aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input datasets: ['st_temazepam']\n",
      "w.shape=(1024, 1024), b.shape=(1024,)\n",
      "st_temazepam.pkl: (265440, 1024)\n",
      "load features\n",
      "feature_train.shape=(1224888, 1024)\n",
      "input.shape= (265440, 1024)\n",
      "computing logits...\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    fc='slp_fc2.pkl',\n",
    "    train_feature='shhs_test.pkl',\n",
    "    feature_to_detect_data=['st_temazepam.pkl']\n",
    ")\n",
    "\n",
    "input_names = [splitext(basename(data))[0] for data in args.feature_to_detect_data]\n",
    "print(f\"input datasets: {input_names}\")\n",
    "\n",
    "w, b = mmcv.load(args.fc)\n",
    "print(f'{w.shape=}, {b.shape=}')\n",
    "\n",
    "for feat in args.feature_to_detect_data:\n",
    "    loaded_data = mmcv.load(feat)\n",
    "    print(f'{feat}: {loaded_data.shape}')\n",
    "\n",
    "print('load features')\n",
    "feature_train = mmcv.load(args.train_feature).squeeze()\n",
    "# 加载数据并获取唯一的 feature_to_detect_data 元素\n",
    "feature_to_detect_data = mmcv.load(args.feature_to_detect_data[0]).squeeze()\n",
    "\n",
    "print(f'{feature_train.shape=}')\n",
    "\n",
    "print(f'input.shape= {feature_to_detect_data.shape}')\n",
    "\n",
    "print('computing logits...')\n",
    "logit_train = feature_train @ w.T + b\n",
    "logit_input_data = [feat @ w.T + b for feat in feature_to_detect_data]\n",
    "\n",
    "u = -np.matmul(pinv(w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ec186b-320c-4ad6-91b7-ab7de9955389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ViM\n",
      "DIM=512\n",
      "computing num_samples...\n",
      "Number of Samples is 265440.\n",
      "computing principal space...\n",
      "EmpiricalCovariance(assume_centered=True)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "method = 'ViM'\n",
    "print(f'\\n{method}')\n",
    "\n",
    "DIM = 1000 if feature_to_detect_data.shape[-1] >= 2048 else 512\n",
    "print(f'{DIM=}')\n",
    "\n",
    "# 计算 feature_to_detect_data 列表中元素的数量\n",
    "print('computing num_samples...')\n",
    "num_samples = len(feature_to_detect_data)\n",
    "print(f'Number of Samples is {num_samples}.')\n",
    "\n",
    "print('computing principal space...')\n",
    "ec = EmpiricalCovariance(assume_centered=True)\n",
    "ec.fit(feature_train - u)\n",
    "print(ec.fit(feature_train - u))\n",
    "\n",
    "eig_vals, eigen_vectors = np.linalg.eig(ec.covariance_)\n",
    "NS = np.ascontiguousarray((eigen_vectors.T[np.argsort(eig_vals * -1)[DIM:]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758ff042-1090-4659-ba3a-4d756da1d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "a = np.matmul(feature_train - u, NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da91fa28-8aac-4a5d-94dd-882bd7c509ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing vlogit...\n",
      "vlogit of train is: [325.71884539 327.28126639 320.43340273 ... 322.49314746 325.8244727\n",
      " 325.9133953 ]\n",
      "vlogit mean of train is: 318.41181346701785\n"
     ]
    }
   ],
   "source": [
    "print('computing vlogit...')\n",
    "# 对每个数据取对数\n",
    "a_log = np.log(np.abs(a) + 1e-12)  # 避免取对数时出现零值\n",
    "vlogit_train =  np.linalg.norm(a_log, axis=1)\n",
    "print(f'vlogit of train is: {vlogit_train}')\n",
    "print(f'vlogit mean of train is: {vlogit_train.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d281a8-b4e8-46cb-8fe6-e52e8b9887e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing alpha...\n",
      "alpha=0.0131\n"
     ]
    }
   ],
   "source": [
    "print('computing alpha...')\n",
    "alpha = logit_train.max(axis=-1).mean() / vlogit_train.mean()\n",
    "print(f'{alpha=:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0727770b-423a-41c7-9fd1-292e9c2e170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vlogit is: 4.272768312308847\n",
      "4.5368104\n",
      "[4.27276831 0.         0.         ... 0.         0.         0.        ]\n",
      "[0.03583892 0.00049971 0.00049971 ... 0.00049971 0.00049971 0.00049971]\n",
      "0.0358389151554449\n",
      "0.0009756097560975609\n"
     ]
    }
   ],
   "source": [
    "feature_input = feature_to_detect_data[17592]\n",
    "vlogit_input = np.linalg.norm(np.log(np.matmul(feature_input - u, NS)+1e-12), axis=-1) * alpha\n",
    "print(f'vlogit is: {vlogit_input}')\n",
    "\n",
    "logit_input=logit_input_data[17592]\n",
    "\n",
    "# 将小于0.1的值设置为0\n",
    "logit_input[logit_input < 1] = 0    \n",
    "\n",
    "all_logits = np.concatenate(([vlogit_input], logit_input))\n",
    "\n",
    "print(logit_input.max())\n",
    "\n",
    "print(all_logits)\n",
    "\n",
    "probabilities = softmax(all_logits, axis=-1)\n",
    "\n",
    "print(probabilities)\n",
    "\n",
    "p0 = probabilities[0]\n",
    "print(p0)\n",
    "print(probabilities.mean())\n",
    "\n",
    "# 判断是否为 OOD 数据\n",
    "is_ood = vlogit_input > logit_input.max()\n",
    "\n",
    "if is_ood:\n",
    "    print(f'Sample is OOD: {is_ood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c59f7e5-a458-44ae-926b-fc23032b9094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vlogit is: 4.272768312308847\n",
      "[4.27276831 0.         0.         ... 0.         0.         0.        ]\n",
      "[0.03583892 0.00049971 0.00049971 ... 0.00049971 0.00049971 0.00049971]\n",
      "0.0358389151554449\n"
     ]
    }
   ],
   "source": [
    "feature_input = feature_to_detect_data[17592]\n",
    "vlogit_input = np.linalg.norm(np.log(np.matmul(feature_input - u, NS) + 1e-12), axis=-1) * alpha\n",
    "print(f'vlogit is: {vlogit_input}')\n",
    "\n",
    "logit_input = logit_input_data[17592]\n",
    "\n",
    "# 合并 vlogit 和 归一化后的 logit 值到一个向量\n",
    "all_logits = np.concatenate(([vlogit_input], logit_input))\n",
    "print(all_logits)\n",
    "\n",
    "# 对合并后的向量进行 softmax 处理\n",
    "probabilities = softmax(all_logits, axis=-1)\n",
    "print(probabilities)\n",
    "\n",
    "# 获取 p0 值\n",
    "p0 = probabilities[0]\n",
    "print(p0)\n",
    "\n",
    "\n",
    "# 判断是否为 OOD 数据\n",
    "is_ood = p0 > 0.8\n",
    "\n",
    "if is_ood:\n",
    "    print(f'Sample is OOD: {is_ood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ff8e32-282f-4575-a56b-5ccfef5d5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOD samples: 130773\n",
      "Over! We have 265440 samples and run 265440 samples\n"
     ]
    }
   ],
   "source": [
    "f1ood_indices_list = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # 获取当前样本的 feature_input\n",
    "    feature_input = feature_to_detect_data[i]\n",
    "    \n",
    "    vlogit_input = np.linalg.norm(np.log(np.matmul(feature_input - u, NS)+1e-12), axis=-1) * alpha\n",
    "\n",
    "    logit_input = logit_input_data[i]\n",
    "\n",
    "    # 合并 vlogit 和 logit 值到一个向量\n",
    "    all_logits = np.concatenate(([vlogit_input], logit_input))\n",
    "\n",
    "    # 对合并后的向量进行 softmax 处理\n",
    "    probabilities = softmax(all_logits, axis=-1)\n",
    "\n",
    "    # 获取 p0 值\n",
    "    p0 = probabilities[0]\n",
    "\n",
    "    # 判断是否为 OOD 数据\n",
    "    is_ood = p0 > 0.038\n",
    "\n",
    "    # Append OOD indices to list if is_ood is True\n",
    "    if is_ood:\n",
    "        f1ood_indices_list.append(i)\n",
    "        # Print the OOD detection result for the current sample\n",
    "        # print(f'Sample {i + 1} is OOD: {is_ood}')\n",
    "\n",
    "    i += 1  # Increment the counter\n",
    "\n",
    "    \n",
    "# Calculate the number of elements in ood_indices_list\n",
    "num_ood_samples = len(f1ood_indices_list)\n",
    "\n",
    "# Print the number of elements in ood_indices_list\n",
    "print(f\"Number of OOD samples: {num_ood_samples}\")\n",
    "    \n",
    "print(f'Over! We have {num_samples} samples and run {i} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56747e26-9ca8-4a11-adfa-c33ebfab1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved OOD indices to f38_ood_indices_list_temazepam.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for the CSV file\n",
    "csv_filename = 'f38_ood_indices_list_temazepam.csv'\n",
    "\n",
    "# Write ood_indices_list to the CSV file\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Index'])  # Write header\n",
    "\n",
    "    for index in f1ood_indices_list:\n",
    "        csv_writer.writerow([index])\n",
    "\n",
    "print(f\"Saved OOD indices to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "374c0a7c-1cd9-492b-a4e3-f917bcb7f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOD samples: 173428\n",
      "Over! We have 265440 samples and run 265440 samples\n"
     ]
    }
   ],
   "source": [
    "#inlier_indices_list = []\n",
    "f2ood_indices_list = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # 获取当前样本的 feature_input\n",
    "    feature_input = feature_to_detect_data[i]\n",
    "    \n",
    "    vlogit_input = np.linalg.norm(np.log(np.matmul(feature_input - u, NS)+1e-12), axis=-1) * alpha\n",
    "\n",
    "    logit_input = logit_input_data[i]\n",
    "\n",
    "    # 合并 vlogit 和 logit 值到一个向量\n",
    "    all_logits = np.concatenate(([vlogit_input], logit_input))\n",
    "\n",
    "    # 对合并后的向量进行 softmax 处理\n",
    "    probabilities = softmax(all_logits, axis=-1)\n",
    "\n",
    "    # 获取 p0 值\n",
    "    p0 = probabilities[0]\n",
    "\n",
    "    # 判断是否为 OOD 数据\n",
    "    is_ood = p0 > 0.035\n",
    "\n",
    "    # Append OOD indices to list if is_ood is True\n",
    "    if is_ood:\n",
    "        f2ood_indices_list.append(i)\n",
    "        # Print the OOD detection result for the current sample\n",
    "        # print(f'Sample {i + 1} is OOD: {is_ood}')\n",
    "    #else:\n",
    "        #inlier_indices_list.append(i)        \n",
    "        \n",
    "    i += 1  # Increment the counter\n",
    "\n",
    "    \n",
    "# Calculate the number of elements in ood_indices_list\n",
    "num_ood_samples = len(f2ood_indices_list)\n",
    "\n",
    "# Print the number of elements in ood_indices_list\n",
    "print(f\"Number of OOD samples: {num_ood_samples}\")\n",
    "    \n",
    "print(f'Over! We have {num_samples} samples and run {i} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6967cb9e-32dc-4067-ba82-3e77fd1ffc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved OOD indices to f35_ood_indices_list_temazepam.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for the CSV file\n",
    "csv_filename = 'f35_ood_indices_list_temazepam.csv'\n",
    "\n",
    "# Write ood_indices_list to the CSV file\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Index'])  # Write header\n",
    "\n",
    "    for index in f2ood_indices_list:\n",
    "        csv_writer.writerow([index])\n",
    "\n",
    "print(f\"Saved OOD indices to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b42652e8-b2b8-474a-a393-8a5943032180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to select inliers from feature_to_detect_data\n",
    "#inlier_mask = np.array([i in inlier_indices_list for i in range(num_samples)])\n",
    "\n",
    "# Select inlier data from feature_to_detect_data\n",
    "#inlier_feature_data = feature_to_detect_data[inlier_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "352a7df2-5805-4047-a8fe-7e47fffb064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inlier_feature_data as \"feature_id.pkl\"\n",
    "#with open('f47_feature_id.pkl', 'wb') as f:\n",
    "#    pickle.dump(inlier_feature_data, f)\n",
    "\n",
    "#print('feature_id.pkl saved successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
